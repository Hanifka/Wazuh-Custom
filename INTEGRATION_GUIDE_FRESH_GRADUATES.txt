FRESH GRADUATE UEBA INTEGRATION GUIDE
=====================================
Updated: 2024-12-02

TABLE OF CONTENTS
-----------------
1. Orientation
2. Architecture Snapshot
3. Step-by-Step Environment Setup
4. Risk Scoring Engine Deep-Dive
   4.1 Algorithm Overview
   4.2 compute_baseline()
   4.3 calculate_daily_risk()
   4.4 Feedback Integration Loop
5. Running the Analyzer Service
6. Inspecting Results
7. Testing & Validation
8. Common Pitfalls & Troubleshooting
9. Integration Checklist
10. Getting Help & Debugging Tips

1. Orientation
--------------
- This guide helps fresh graduates bootstrap the User and Entity Behavior Analytics (UEBA) stack from zero to a functioning analyzer loop.
- You will learn how to prepare the environment, understand the baseline deviation algorithm, run the analyzer in both one-shot and scheduled modes, and verify outputs end-to-end.
- Keep this file nearby during your first week; it consolidates every command needed for local success.

2. Architecture Snapshot
------------------------
- Mapper Service (ingests → normalizes) feeds Analyzer Service (scores → persists).
- Core analyzer modules:
  - `src/ueba/services/analyzer/pipeline.py` → feature extraction, rule evaluation, risk scoring.
  - `src/ueba/services/analyzer/repository.py` → database reads/writes plus checkpointing.
  - `src/ueba/services/analyzer/service.py` → orchestration for one-shot or daemon modes.
  - `src/ueba/services/analyzer/analyzer_service.py` → CLI entry point (`python -m ...`).
- Database tables involved: `normalized_events`, `entity_risk_history`, `tp_fp_feedback`, and optional `threshold_overrides`.
- Risk scoring follows the Baseline Deviation method: 7-day baselines, rule-driven adjustments, and per-entity thresholds.

3. Step-by-Step Environment Setup
---------------------------------
Step 1 – Confirm prerequisites
- Python 3.9 or later (`python3 --version`).
- Git, SQLite (bundled on macOS/Linux), and make.
- Access to the UEBA repository and branch `feat/fresh-grad-integration-guide`.

Step 2 – Clone repository & check branch
```bash
git clone <your-fork-url> ueba
cd ueba
git checkout feat/fresh-grad-integration-guide
```

Step 3 – Configure environment variables
```bash
cp .env.example .env
# Edit .env if you need PostgreSQL. Default SQLite path is ./ueba.db
```

Step 4 – Create a Python virtual environment
```bash
python3 -m venv venv
```

Step 5 – Activate venv & update pip
```bash
source venv/bin/activate
pip install --upgrade pip
```

Step 6 – Install project dependencies
```bash
make setup
# (Equivalent to: pip install SQLAlchemy, Alembic, python-dotenv, pytest, etc.)
```

Step 7 – Initialize the database schema
```bash
make db-upgrade
# Creates ueba.db (SQLite) and runs all Alembic migrations.
```

Step 8 – Smoke-test the analyzer CLI
```bash
python -m ueba.services.analyzer.analyzer_service --mode once --log-level DEBUG
```
- Successful run logs "Analyzer processed X window(s)". If zero, add fixtures or run mapper first.
- Re-run `make db-upgrade` whenever new migrations land.

4. Risk Scoring Engine Deep-Dive
--------------------------------
4.1 Algorithm Overview
- **Baseline Deviation**: build a rolling 7-day baseline (mean + standard deviation) per entity using `entity_risk_history`.
- **Feature Extraction**: count events, capture highest severity, last observed timestamp, and event types (`SimpleFeatureExtractor`).
- **Rule Evaluation**: apply threshold rules (e.g., >10 events → `high_event_volume`, severity ≥8 → `high_severity_detected`).
- **Risk Score Combination**: start with event-count score (capped at 40), add severity bonus (up to 30), then add 30 points per triggered rule. Cap final score at 100.
- **Threshold Decision**: compare today's risk score against baseline mean. Raise alerts when the delta exceeds configurable multiples of the baseline deviation (σ), defaulting to 1.5σ.

4.2 compute_baseline()
- Purpose: calculate the 7-day average and deviation for past risk scores of a single entity.
- Implementation example:
```python
from __future__ import annotations

from datetime import datetime, timedelta, timezone
from statistics import mean, pstdev
from typing import Tuple

from sqlalchemy import select
from sqlalchemy.orm import Session

from ueba.db.base import SessionLocal
from ueba.db.models import EntityRiskHistory


def compute_baseline(session: Session, entity_id: int, days: int = 7) -> Tuple[float, float]:
    """Return (average, sigma) of recent risk scores. Falls back to (0.0, 1.0)."""
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    stmt = (
        select(EntityRiskHistory.risk_score)
        .where(
            EntityRiskHistory.entity_id == entity_id,
            EntityRiskHistory.observed_at >= cutoff,
            EntityRiskHistory.deleted_at.is_(None),
        )
        .order_by(EntityRiskHistory.observed_at.desc())
    )
    scores = session.execute(stmt).scalars().all()
    if not scores:
        return 0.0, 1.0

    baseline_avg = mean(scores)
    baseline_sigma = pstdev(scores) or 1.0
    return baseline_avg, baseline_sigma


with SessionLocal() as session:
    avg, sigma = compute_baseline(session, entity_id=42)
    print(f"Entity 42 baseline → avg={avg:.2f}, σ={sigma:.2f}")
```
- Why it matters: analyzer outputs are only meaningful when you know whether today's score deviates from normal behavior.

4.3 calculate_daily_risk()
- Purpose: run the analyzer pipeline for one entity/UTC day, then compare it to the baseline computed above.
- Implementation example (reuses `compute_baseline`):
```python
from datetime import datetime, timedelta, timezone

from sqlalchemy import select
from sqlalchemy.orm import Session

from ueba.db.base import SessionLocal
from ueba.db.models import NormalizedEvent
from ueba.services.analyzer.pipeline import AnalyzerPipeline

# Reuse compute_baseline from the previous snippet.

def calculate_daily_risk(session: Session, entity_id: int, day: datetime):
    window_start = day.astimezone(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
    window_end = window_start + timedelta(days=1)

    stmt = (
        select(NormalizedEvent)
        .where(
            NormalizedEvent.entity_id == entity_id,
            NormalizedEvent.observed_at >= window_start,
            NormalizedEvent.observed_at < window_end,
            NormalizedEvent.deleted_at.is_(None),
            NormalizedEvent.status == "active",
        )
        .order_by(NormalizedEvent.observed_at)
    )

    events = session.execute(stmt).scalars().all()
    if not events:
        return None

    pipeline = AnalyzerPipeline()
    result = pipeline.analyze(
        entity_id=entity_id,
        window_start=window_start,
        window_end=window_end,
        events=events,
    )

    baseline_avg, baseline_sigma = compute_baseline(session, entity_id)
    delta = result.risk_score - baseline_avg
    is_anomalous = delta >= baseline_sigma * 1.5

    return {
        "result": result,
        "baseline_avg": baseline_avg,
        "baseline_sigma": baseline_sigma,
        "delta": delta,
        "is_anomalous": is_anomalous,
    }


with SessionLocal() as session:
    snapshot = calculate_daily_risk(session, entity_id=42, day=datetime.now(timezone.utc))
    if snapshot is None:
        print("No events for entity 42 today." )
    elif snapshot["is_anomalous"]:
        print(f"Escalate entity 42 → {snapshot['delta']:.1f} above baseline")
    else:
        print("Entity 42 remains within expected bounds.")
```
- Tuning thresholds: replace `1.5` with entity-specific multipliers using `threshold_overrides` when sensitive users need faster alerts.

4.4 Feedback Integration Loop
- Analysts log verdicts in the `tp_fp_feedback` table so future baselines learn from true/false positives.
- Recommended workflow:
  1. Pull the offending `entity_risk_history` row.
  2. Insert analyst feedback referencing the same entity (optionally the `normalized_event_id`).
  3. Nightly jobs can down-weight baseline contributions from rows marked as false positives.
- Example entry:
```python
from datetime import datetime, timezone

from ueba.db.base import SessionLocal
from ueba.db.models import TPFPFeedback

with SessionLocal() as session:
    feedback = TPFPFeedback(
        entity_id=42,
        feedback_type="fp",  # or "tp"
        notes="No lateral movement detected after investigation.",
        submitted_by="analyst.alex",
        submitted_at=datetime.now(timezone.utc),
    )
    session.add(feedback)
    session.commit()
```
- Tip: store links to ticketing systems in `notes` so future reviewers understand the decision context.

5. Running the Analyzer Service
-------------------------------
Manual one-shot run (cron-friendly):
```bash
python -m ueba.services.analyzer.analyzer_service --mode once --log-level INFO
```
Process a custom backfill window:
```bash
python -m ueba.services.analyzer.analyzer_service \
    --mode once \
    --since 2024-01-01T00:00:00Z \
    --until 2024-01-08T00:00:00Z \
    --log-level DEBUG
```
Long-running (daemon) mode sampled every 5 minutes:
```bash
python -m ueba.services.analyzer.analyzer_service --mode daemon --interval 300
```
Scheduling via cron (UTC recommended):
```bash
# Run every 15 minutes. Update paths to match your workstation.
*/15 * * * * /home/you/ueba/venv/bin/python -m ueba.services.analyzer.analyzer_service --mode once >> /home/you/ueba/logs/analyzer.log 2>&1
```
- Ensure the cron environment exports `DATABASE_URL` (set it in `.env` and source it in your cron wrapper script).

6. Inspecting Results
---------------------
6.1 Direct database queries
- Quick peek with SQLite:
```bash
sqlite3 ueba.db <<'SQL'
.mode column
.headers on
SELECT entity_id, risk_score, observed_at, json_extract(reason, '$.rules.triggered') AS rules
FROM entity_risk_history
ORDER BY observed_at DESC
LIMIT 5;
SQL
```
- Python inspection:
```python
from sqlalchemy import select

from ueba.db.base import SessionLocal
from ueba.db.models import EntityRiskHistory

with SessionLocal() as session:
    stmt = select(EntityRiskHistory).order_by(EntityRiskHistory.observed_at.desc()).limit(3)
    for row in session.execute(stmt).scalars():
        print(row.entity_id, row.risk_score, row.reason)
```

6.2 Future REST API example
- The upcoming UEBA API service will expose risk history via HTTP. Expected shape:
```bash
curl -s "http://localhost:8080/api/v1/entities/42/risk-history?since=2024-01-01T00:00:00Z&until=2024-01-02T00:00:00Z"
```
Sample response payload (planned):
```json
{
  "entity_id": 42,
  "windows": [
    {
      "window_start": "2024-01-01T00:00:00Z",
      "window_end": "2024-01-02T00:00:00Z",
      "risk_score": 78.5,
      "baseline_avg": 32.0,
      "baseline_sigma": 8.4,
      "rules": ["high_event_volume", "high_severity_detected"],
      "last_observed_at": "2024-01-01T21:14:00Z"
    }
  ]
}
```
- Until the REST layer ships, rely on the direct database queries above or wrap them in internal scripts.

7. Testing & Validation
-----------------------
- Run the core analyzer suites:
```bash
pytest tests/test_analyzer_pipeline.py
pytest tests/test_analyzer_repository.py
pytest tests/test_analyzer_service.py
```
- Targeted development loop (example: only pipeline tests with verbose output):
```bash
pytest tests/test_analyzer_pipeline.py -vv -k "risk"
```
- Add new fixtures under `tests/fixtures/` when simulating novel event patterns.
- Tests rely on in-memory SQLite, so they are fast—run them before every pull request.

8. Common Pitfalls & Troubleshooting
------------------------------------
+----+-----------------------------------------------+------------------------------------------------+---------------------------------------------------------------+
| #  | Issue                                         | Symptom                                        | Fix                                                           |
+----+-----------------------------------------------+------------------------------------------------+---------------------------------------------------------------+
| 1  | `.env` missing or DATABASE_URL unset          | `sqlalchemy.exc.NoSuchModuleError` on startup  | Copy `.env.example`, export `DATABASE_URL`, re-run CLI.        |
| 2  | Virtual env not activated                     | `ModuleNotFoundError: No module named 'ueba'`  | `source venv/bin/activate` before running python/pytest.       |
| 3  | Migrations not applied                        | `no such table: normalized_events` errors      | Execute `make db-upgrade` and confirm `ueba.db` contains tables.|
| 4  | Analyzer run outside UTC                      | Duplicate/missing daily windows                | Always run in UTC; use cron on UTC hosts or call `tzset`.      |
| 5  | Empty baselines (new entity)                  | Baseline sigma = 1.0 fallback, noisy alerts    | Seed history with mapper fixtures or widen baseline window.    |
| 6  | SQLite locked by another process              | `database is locked` during analyzer writes    | Close extra shells, stop GUI viewers, or switch to PostgreSQL. |
+----+-----------------------------------------------+------------------------------------------------+---------------------------------------------------------------+

9. Integration Checklist
------------------------
[ ] Repository cloned and branch `feat/fresh-grad-integration-guide` checked out.
[ ] Python 3.9+ verified along with make and SQLite.
[ ] Virtual environment created (`python3 -m venv venv`) and activated.
[ ] Dependencies installed via `make setup` (pip upgraded).
[ ] `.env` configured with the correct `DATABASE_URL`.
[ ] Database migrations executed (`make db-upgrade`).
[ ] Analyzer CLI completes a one-shot run without errors.
[ ] `compute_baseline()` returns sensible values for a sample entity.
[ ] `calculate_daily_risk()` identifies at least one valid window and persists results.
[ ] Pytest suite (`tests/test_analyzer_*.py`) runs cleanly before code review.

10. Getting Help & Debugging Tips
---------------------------------
- Increase verbosity with `--log-level DEBUG` to view checkpoint decisions and window summaries.
- Use `sqlite3 ueba.db "SELECT COUNT(*) FROM normalized_events;"` to confirm mapper output exists before blaming the analyzer.
- When daemon mode seems stuck, tail the log file and ensure the cron/daemon user sources `.env`.
- Reproduce issues with targeted pytest cases; they create isolated SQLite DBs and are ideal for regression tests.
- Capture TP/FP notes whenever resolving alerts—those entries shorten future investigations.
- If you are blocked, share analyzer logs (`logs/analyzer.log`), the output of `alembic current`, and any custom mapping files with your mentor or the platform Slack channel `#ueba-dev`.
